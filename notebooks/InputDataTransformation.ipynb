{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkIVLFZniyB4"
   },
   "source": [
    "# Find Your Rhythm Input Data Transformation\n",
    "\n",
    "Transform audio input and label data into arrays for consumption into an Neural Network ML model\n",
    "\n",
    "Labeled input data sources:\n",
    "\n",
    "* MDB\n",
    "* IDMT-SMT\n",
    "* e-GMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7heIO3tikK-"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import librosa\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import sys\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility functions\n",
    "sys.path.append('../progs')\n",
    "from utility_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSaB0n8TjfbN"
   },
   "source": [
    "As all labels will be standardized to a MIDI pitch, define the drum MIDI pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Glmm1R7TjSQS"
   },
   "outputs": [],
   "source": [
    "midi_classes = {35: 'Acoustic Bass Drum', 36: 'Bass Drum 1', 37: 'Side Stick', 38: 'Acoustic Snare', 39: 'Hand Clap', \n",
    "                40: 'Electric Snare', 41: 'Low Floor Tom', 42: 'Closed Hi Hat', 43: 'High Floor Tom', 44: 'Pedal Hi-Hat', \n",
    "                45: 'Low Tom', 46: 'Open Hi-Hat', 47: 'Low-Mid Tom', 48: 'Hi-Mid Tom', 49: 'Crash Cymbal 1', \n",
    "                50: 'High Tom',  51: 'Ride Cymbal 1', 52: 'Chinese Cymbal', 53: 'Ride Bell', 54: 'Tambourine', \n",
    "                55: 'Splash Cymbal', 56: 'Cowbell', 57: 'Crash Cymbal 2', 58: 'Vibraslap', 59: 'Ride Cymbal 2', \n",
    "                60: 'Hi Bongo', 61: 'Low Bongo', 62: 'Mute Hi Conga', 63: 'Open Hi Conga', 64: 'Low Conga', \n",
    "                65: 'High Timbale', 66: 'Low Timbale', 67: 'High Agogo', 68: 'Low Agogo', 69: 'Cabasa', \n",
    "                70: 'Maracas', 71: 'Short Whistle', 72: 'Long Whistle', 73: 'Short Guiro', 74: 'Long Guiro', \n",
    "                75: 'Claves', 76: 'Hi Wood Block', 77: 'Low Wood Block', 78: 'Mute Cuica', 79: 'Open Cuica', \n",
    "                80: 'Mute Triangle', 81: 'Open Triangle',\n",
    "               # to be removed\n",
    "                22: 'Unknown 22', 26: 'Unknown 26'\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpzdzPRLjvgM"
   },
   "source": [
    "Read in the labels for each input dataset\n",
    "\n",
    "Labels take the structure:\n",
    "\n",
    "_{source: [song index, [onset time, midi pitch, offset time, duration, velocity, time signature numerator, time signature denominator, beat number, measure number]]}_\n",
    "\n",
    "with each song_index refering to the order in which a song was labeled and serving as a key to the song-index dictionary. If a particular label element is not available in a song annotation file, then the element is assigned a Numpy 'nan' value.\n",
    "\n",
    "The song-index dictionary takes the structure:\n",
    "\n",
    "*{source: {song index: audio file name}}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in label dict/list from GCP\n",
    "from google.cloud import storage\n",
    "\n",
    "auth_json = '../gcp_bucket_auth/fyr-bucket-auth.json'\n",
    "bucket_name = 'fyr-audio-data'\n",
    "gcp_filepath = 'raw-audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BY9xMwPjTGr"
   },
   "outputs": [],
   "source": [
    "song_dict_files = {'idmt': 'IDMTdict.pkl','mdb': 'MDBdict.pkl','egmd': 'eGMDdict.pkl'}\n",
    "                   #'sf': 'SoundFontFiles/eGMDdict.pkl'}\n",
    "song_list_files = {'idmt': 'IDMTlabels.pkl', 'mdb': 'MDBlabels.pkl','egmd': 'eGMDlabels.pkl'}\n",
    "                   #'sf': 'SoundFontFiles/eGMDlabels.pkl'}\n",
    "\n",
    "label_data = {}\n",
    "label_dict = {}\n",
    "\n",
    "for source in song_dict_files:\n",
    "    label_data_filepath = gcp_filepath + song_list_files[source]\n",
    "    label_dict_filepath = gcp_filepath + song_dict_files[source]\n",
    "    \n",
    "    label_data[source] = readGcpPkl(auth_json, bucket_name, label_data_filepath)\n",
    "    label_dict[source] = readGcpPkl(auth_json, bucket_name, label_dict_filepath)\n",
    "\n",
    "# hack for MDB file suffix\n",
    "mdb_suffix = '_Drum'\n",
    "\n",
    "for i in range(len(label_dict['mdb'])):\n",
    "    label_dict['mdb'][i] += mdb_suffix\n",
    "    \n",
    "# hack for IDMT file suffix\n",
    "for i in range(len(label_dict['idmt'])):\n",
    "    # remove .xml suffix from IDMT file names\n",
    "    label_dict['idmt'][i] = label_dict['idmt'][i].replace('.xml','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update label data items to be dict instead of list\n",
    "# necessary for indexing that is not dependent on order of list items\n",
    "#egmd_data_dict = {item[0] : item[1] for item in label_data['egmd'] if item[0] < 40000}\n",
    "#sf_data_dict = {item[0] : item[1] for item in label_data['sf']}\n",
    "idmt_data_dict = {item[0] : item[1] for item in label_data['idmt']}\n",
    "mdb_data_dict = {item[0] : item[1] for item in label_data['mdb']}\n",
    "\n",
    "#label_data['egmd'] = egmd_data_dict #eGMD files have been fixed\n",
    "#label_data['sf'] = sf_data_dict\n",
    "label_data['idmt'] = idmt_data_dict\n",
    "label_data['mdb'] = mdb_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick label count\n",
    "\n",
    "label_counts = {key: 0 for key in midi_classes.keys()}\n",
    "\n",
    "for source in label_data:\n",
    "    for audio in label_data[source].values():\n",
    "        for labels in audio:\n",
    "            label_counts[labels[1]] += 1\n",
    "            \n",
    "{midi_classes[k]: v for k, v in sorted(label_counts.items(), key=lambda item: item[1], reverse=True) if v > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynL_j4ZkhWB5"
   },
   "source": [
    "### Function for creating numpy arrays from raw data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EPe_SrjhKmS"
   },
   "outputs": [],
   "source": [
    "def audioDataTransform(wav_file, auth_json, bucket,\n",
    "                       source, representation, \n",
    "                       labels, label_data, label_dict,\n",
    "                       segment_length=5, sr=22050):\n",
    "    \"\"\"\n",
    "    Takes as input a GCP path to wav file, GCP auth json, bucket name,\n",
    "    data source, array representation type, \n",
    "    instrument labels, label data, song index dict,\n",
    "    segment length in seconds, and sampling rate to generate numpy arrays\n",
    "    used for input into neural net models\n",
    "    \n",
    "    wav_file = GCP path to wav file\n",
    "    auth_json = str filepath to json authentication for GCP bucket read/writes\n",
    "    bucket = str bucket name\n",
    "    source = 'mdb', 'idmt', or 'egmd' - necessary due to different \n",
    "             file name formats\n",
    "    representation = 'stft': short time fourier transform, \n",
    "                     'cqt': constant Q transform\n",
    "                     'mel_stft': log-based mel spectrogram\n",
    "    labels = list of unique instrument labels (MIDI pitches)\n",
    "    label_data = dictionary of annotated labels\n",
    "    label_dict = dictionary of song indexes and song names\n",
    "    segmentLength = length of each segment, in seconds (default is 5 seconds)\n",
    "    sr = sampling rate (default is 22050)\n",
    "    \n",
    "    \n",
    "    Outputs:\n",
    "    arr_out = amplitude array of shape (number of frames,  frequency bins)\n",
    "    label_out = Onset array of shape (number of frames,  instrument onsets)\n",
    "    \"\"\"\n",
    "       \n",
    "    # convert segmentLength into integer if a float value is provided\n",
    "    segment_length = int(segment_length)\n",
    "    # load the wav file using utility function into an audio time series array\n",
    "    try:\n",
    "        wav = readGcpWav(auth_json, bucket, wav_file, sr = sr)\n",
    "    except:\n",
    "        print('Unable to read file:', wav_file)\n",
    "        return np.empty(0), np.empty(0)\n",
    "        \n",
    "    # length of the full wav array\n",
    "    wav_len = len(wav)\n",
    "    # default feature parameters\n",
    "    n_fft = 1024\n",
    "    hop_length = 512\n",
    "    \n",
    "    # create an array representation of the wav audio data\n",
    "    if representation == 'stft':\n",
    "        # transpose the stft array so that rows represent each frame in time and features (columns) are frequency bins\n",
    "        data = np.absolute(librosa.stft(wav)).transpose()\n",
    "        \n",
    "    elif representation == 'cqt':\n",
    "        # transpose the cqt array so that rows represent each frame in time and features (columns) are frequency bins\n",
    "        data = np.absolute(librosa.cqt(wav, sr=sr)).transpose()\n",
    "        \n",
    "    elif representation == 'mel_stft':\n",
    "        # transpose the mel stft array so that rows represent each frame in time and features (columns) are frequency bins\n",
    "        # parameters are borrowed from the paper here: https://goo.gl/magenta/e-gmd-paper\n",
    "        sr = 44100\n",
    "        n_fft = 2048\n",
    "        hop_length = 441\n",
    "        data = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=250).transpose()\n",
    "        \n",
    "    # pad the end of the audio with silence depending on the sequence length\n",
    "    number_frames = data.shape[0]\n",
    "    segment_length_frames = librosa.time_to_frames(segment_length, sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
    "    # calculate how much silence should be added in last segment\n",
    "    remaining_frames = int(segment_length_frames-(number_frames % segment_length_frames))\n",
    "    # if the audio can not fill the frames of last segment, silence is padded on to the end of the segment\n",
    "    if remaining_frames != segment_length_frames:\n",
    "        # attach zeros to the end of row of stft to make sure two songs are not merged within one segment \n",
    "        data = np.append(data,np.zeros((remaining_frames,data.shape[1])),axis=0)\n",
    "        \n",
    "    \n",
    "    # create an array representation of audio onset labels\n",
    "    # label array shape will be (number_frames, number_labels) \n",
    "    # each array in the second dimension (number_labels) will have 1 if there is an onset for that instrument\n",
    "\n",
    "    # create an array of zeroes to match the length of the data array\n",
    "    if remaining_frames != segment_length_frames:\n",
    "        label_arr = np.zeros((number_frames + remaining_frames,len(labels)))\n",
    "    else:\n",
    "        label_arr = np.zeros((number_frames,len(labels)))\n",
    "        \n",
    "    # find the corresponding labels for the given wav_file\n",
    "    song_index = [key for key in label_dict[source] if label_dict[source][key] == wav_file.split('/')[-1][:-4]][0]\n",
    "        \n",
    "    # enumerate labels so that instrument array can be indexed\n",
    "    label_to_int = {k:v for v,k in enumerate(labels)}\n",
    "    \n",
    "    # given a song index, iterate through each label to get the onset time and midi pitch\n",
    "    for label in label_data[source][song_index]:\n",
    "        # label array is formated in frames, so convert onset time to frame index value\n",
    "        index_row = librosa.time_to_frames(label[0],sr=sr)\n",
    "        # find column number for the drum instrument type based on enumerated labels (label_to_int)\n",
    "        index_col = label_to_int[label[1]]\n",
    "        # update the label array with a value of 1 at the:\n",
    "        #        frame associated with the onset time (first dimension of label_arr)\n",
    "        #        array associated with the midi label (second dimension of label_arr)\n",
    "        \n",
    "        ### FIX FOR LABELS WITH ONSET TIMES OUTSIDE OF THE AUDIO LENGTH###\n",
    "        if index_row < label_arr.shape[0]:\n",
    "            label_arr[index_row,index_col] = 1\n",
    "    \n",
    "    \n",
    "    arr_out = np.asarray(data)\n",
    "    label_out = np.asarray(label_arr)\n",
    "    return arr_out, label_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6UhXwVwjf8u"
   },
   "source": [
    "### Functions for splitting MDB, IDMT, and e-GMD files into training, validation, and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rC2_gIpEjti_"
   },
   "outputs": [],
   "source": [
    "# GCP bucket path to wav files\n",
    "egmd_wavfile_dir = 'raw-audio/eGMD/eGMD-wavfiles/'\n",
    "egmd_sf_wavfile_dir = 'raw-audio/eGMD/eGMD-SoundFont-wavfiles/'\n",
    "mdb_wavfile_dir = 'raw-audio/IDMT/'\n",
    "idmt_wavfile_dir = 'raw-audio/MDBDrums/'\n",
    "\n",
    "# get file names from GCP bucket\n",
    "storage_client = storage.Client.from_service_account_json(auth_json)\n",
    "\n",
    "egmd_wavfiles = [blob.name for blob in storage_client.list_blobs(bucket_name,prefix='raw-audio/eGMD/') \n",
    "                 if blob.name[-4:] == '.wav' \n",
    "                 and blob.name.split('/')[-1][:-4] in label_dict['egmd'].values() # only pull files in label dict\n",
    "                 and blob.name.split('/')[-2] != 'sr9000' # ignore degraded sample rate files\n",
    "                 # genre and beat restrictions for testing\n",
    "                 and blob.name.split('_')[-3] == 'beat'\n",
    "                 and blob.name.split('_')[-5][:11] == 'soul-groove'\n",
    "                ] \n",
    "\n",
    "#egmd_sf_wavfiles = [blob.name for blob in storage_client.list_blobs(bucket_name,prefix=egmd_sf_wavfile_dir)\n",
    "#                    if blob.name[-4:] == '.wav']\n",
    "\n",
    "mdb_wavfiles = [blob.name for blob in storage_client.list_blobs(bucket_name,prefix=mdb_wavfile_dir)\n",
    "                if blob.name[-4:] == '.wav']\n",
    "\n",
    "idmt_wavfiles = [blob.name for blob in storage_client.list_blobs(bucket_name,prefix=idmt_wavfile_dir)\n",
    "                if blob.name[-4:] == '.wav']\n",
    "\n",
    "# combine all files\n",
    "combined_wavfiles = mdb_wavfiles + idmt_wavfiles + egmd_wavfiles # + egmd_sf_wavfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMk8-IotmcNM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def fileShuffleSplit(wavfiles, train_p=0.7, val_p=0.2, test_p=0.1):\n",
    "    \"\"\"\n",
    "    Given an input list of wav files, shuffle the list and output datasets corresponding to the proportion parameters\n",
    "    \n",
    "    Inputs:\n",
    "      wavfiles = list of .wav files\n",
    "      train_p = proportion of the .wav files that should be used in the training dataset\n",
    "      train_p = proportion of the .wav files that should be used in the training dataset\n",
    "      train_p = proportion of the .wav files that should be used in the training dataset\n",
    "      \n",
    "    Outputs:\n",
    "      train_wavfiles = slice of wavfiles list corresponding to the train proportion parameter \n",
    "      val_wavfiles = slice of wavfiles list corresponding to the validation proportion parameter\n",
    "      test_wavfiles = slice of wavfiles list corresponding to the test proportion parameter\n",
    "    \"\"\"\n",
    "    if round(train_p + val_p + test_p, 2) != 1.0:\n",
    "        raise ValueError('Split proportions train_p, val_p, test_p must total to 1.0')\n",
    "    \n",
    "    random.shuffle(wavfiles)\n",
    "\n",
    "    train_index = int(len(wavfiles) * train_p)\n",
    "    val_index = int(len(wavfiles) * val_p)\n",
    "\n",
    "    train_wavfiles = wavfiles[:train_index]\n",
    "    val_wavfiles = wavfiles[train_index : (train_index + val_index)]\n",
    "    test_wavfiles = wavfiles[(train_index + val_index) :]\n",
    "\n",
    "    if len(wavfiles) != (len(train_wavfiles) + len(val_wavfiles) + len(test_wavfiles)):\n",
    "        raise ValueError('Train-Validation-Test file splits do not sum to total number of files')\n",
    "    else:\n",
    "        return train_wavfiles, val_wavfiles, test_wavfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNTMFs7GmdPE"
   },
   "outputs": [],
   "source": [
    "def audioDataArrays(wavfiles, auth_json, bucket, representation,\n",
    "                   labels, label_data, label_dict,\n",
    "                   segment_length=5, sr=22050):\n",
    "    \"\"\"\n",
    "    Given a list of wavfiles, representation, labels list, label_data, label_dict,\n",
    "    segment length, and sample rate, output training, validation, test\n",
    "    datasets with labels and audio look-up dicts\n",
    "\n",
    "    Inputs:\n",
    "    wav_file = GCP path to wav file\n",
    "    auth_json = str filepath to json authentication for GCP bucket read/writes\n",
    "    bucket = str bucket name\n",
    "    representation = 'stft': short time fourier transform, \n",
    "                    'cqt': constant Q transform\n",
    "                    (default is 'stft')\n",
    "    labels = list of unique instrument labels (MIDI pitches)\n",
    "    label_data = dictionary of annotated labels\n",
    "    label_dict = dictionary of song indexes and song names\n",
    "    segmentLength = length of each segment, in seconds (default is 5 seconds)\n",
    "    sr = sampling rate (default is 22050)\n",
    "\n",
    "    Outputs:\n",
    "    One of each of the below for training, validation, and test\n",
    "    (total output of 9 objects)\n",
    "\n",
    "    arr_x = an array of concatenated data from all wavfiles \n",
    "          shape: (frames, frequency bins)\n",
    "    arr_y = an array of concatenated labels from all wavfiles\n",
    "          shape: (frames, instrument labels)\n",
    "\n",
    "    arr_dict = an audio name index dict: {audio first frame: audio name}\n",
    "    \"\"\"\n",
    "    trainfiles, valfiles, testfiles = fileShuffleSplit(wavfiles, train_p=0.8, val_p=0.1, test_p=0.1)\n",
    "\n",
    "    def arrayOutputs(input_files):\n",
    "        \"\"\"\n",
    "        Function to help with the repetitive nature of outputing seperate \n",
    "        arrays for training, validation, and test datasets\n",
    "\n",
    "        Inputs:\n",
    "        input_files = list of wav files\n",
    "\n",
    "        Outputs:\n",
    "        arr_x = an array of concatenated data from all wavfiles \n",
    "              shape: (frames, frequency bins)\n",
    "        arr_y = an array of concatenated labels from all wavfiles\n",
    "              shape: (frames, instrument labels)\n",
    "\n",
    "        arr_dict = an audio name index dict: {audio first frame: audio name}\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        arr_dict = {}\n",
    "\n",
    "        for file in input_files:\n",
    "            if file[-9:] == '_Drum.wav': # MDB files\n",
    "                source = 'mdb'\n",
    "            elif file[-8:] == '#MIX.wav': # IDMT files\n",
    "                source = 'idmt'\n",
    "            #elif file[-9:] == '.midi.wav' and file.split('/')[-2][:2] in('SF','sr'): \n",
    "            # egmd SoundFile / reduced sampling rate files\n",
    "            #    source = 'sf'\n",
    "            elif file[-9:] == '.midi.wav': # eGMD files\n",
    "                #wav_file = egmd_wavfile_dir + file\n",
    "                source = 'egmd'\n",
    "\n",
    "            input_data, input_labels = audioDataTransform(wav_file=file, auth_json=auth_json, bucket=bucket,\n",
    "                                                          source=source, representation = representation, \n",
    "                                                          labels = labels, label_data = label_data, label_dict = label_dict,\n",
    "                                                          segment_length=segment_length, sr=sr)\n",
    "            \n",
    "            if input_data.shape != (0,): # if there was not an empty array returned\n",
    "                if i == 0:\n",
    "                    arr_x = input_data\n",
    "                    arr_y = input_labels\n",
    "                    audio_first_frame = 0\n",
    "                    i += 1\n",
    "                    print('{}% complete. {} out of {} files'.format(\n",
    "                        round((i/len(input_files))*100,2),i,len(input_files)\n",
    "                         ))\n",
    "                else:\n",
    "                    arr_x = np.concatenate((arr_x, input_data), axis=0)\n",
    "                    arr_y = np.concatenate((arr_y, input_labels), axis=0)\n",
    "                    #audio_first_frame += input_data.shape[0]\n",
    "                    i += 1\n",
    "                    print('{}% complete. {} out of {} files'.format(\n",
    "                        round((i/len(input_files))*100,2),i,len(input_files)\n",
    "                         ))\n",
    "\n",
    "                arr_dict.update({audio_first_frame : file})\n",
    "                audio_first_frame += input_data.shape[0]\n",
    "\n",
    "        return arr_x, arr_y, arr_dict\n",
    "\n",
    "\n",
    "    # utitilize the above function for training, validation, and test\n",
    "    # array outputs locally (will be uploaded to GCP and cleaned up)\n",
    "    os.mkdir('model_inputs/')\n",
    "\n",
    "    train_x, train_y, train_dict = arrayOutputs(trainfiles)\n",
    "    # training dataset outputs\n",
    "    np.save('model_inputs/train_x.npy', train_x)\n",
    "    np.save('model_inputs/train_y.npy', train_y)\n",
    "    pickle.dump(train_dict, open(\"model_inputs/train_dict.pkl\", \"wb\"))\n",
    "\n",
    "    del train_x, train_y, train_dict\n",
    "\n",
    "    val_x, val_y, val_dict = arrayOutputs(valfiles)\n",
    "    # validation dataset outputs\n",
    "    np.save('model_inputs/val_x.npy', val_x)\n",
    "    np.save('model_inputs/val_y.npy', val_y)\n",
    "    pickle.dump(val_dict, open(\"model_inputs/val_dict.pkl\", \"wb\" ))\n",
    "\n",
    "    del val_x, val_y, val_dict\n",
    "\n",
    "\n",
    "    test_x, test_y, test_dict = arrayOutputs(testfiles)\n",
    "    # test dataset outputs\n",
    "    np.save('model_inputs/test_x.npy', test_x)\n",
    "    np.save('model_inputs/test_y.npy', test_y)\n",
    "    pickle.dump(test_dict, open(\"model_inputs/test_dict.pkl\", \"wb\" ))\n",
    "\n",
    "    del test_x, test_y, test_dict\n",
    "    \n",
    "    # output labels as pkl file\n",
    "    pickle.dump(list(labels), open(\"model_inputs/labels.pkl\", \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1Oz-U0x7nDUs",
    "outputId": "e71f2747-5cc2-46ef-fec1-553d1b87a1c9"
   },
   "outputs": [],
   "source": [
    "# adjust number of eGMD files\n",
    "n = 10\n",
    "random.shuffle(egmd_wavfiles)\n",
    "#combined_wavfiles = mdb_wavfiles + idmt_wavfiles + egmd_sf_wavfiles[:n] # + egmd_wavfiles[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Closed Hi Hat': 1252,\n",
       " 'Acoustic Bass Drum': 616,\n",
       " 'Acoustic Snare': 525,\n",
       " 'Tambourine': 384,\n",
       " 'Side Stick': 86,\n",
       " 'High Tom': 12,\n",
       " 'Low-Mid Tom': 8,\n",
       " 'Open Hi-Hat': 4,\n",
       " 'Pedal Hi-Hat': 2}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = {key: 0 for key in midi_classes.keys()}\n",
    "\n",
    "for audio in egmd_wavfiles[:n]:\n",
    "    song_index = [key for key in label_dict['egmd'] if label_dict['egmd'][key] == audio.split('/')[-1][:-4]][0]\n",
    "\n",
    "    for labels in label_data['egmd'][song_index]:\n",
    "        label_counts[labels[1]] += 1\n",
    "            \n",
    "{midi_classes[k]: v for k, v in sorted(label_counts.items(), key=lambda item: item[1], reverse=True) if v > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rdy6tW37rf9Q",
    "outputId": "8f5b0614-5630-4ec2-9c1f-ea85fba6b0c8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5% complete. 1 out of 8 files\n",
      "25.0% complete. 2 out of 8 files\n",
      "37.5% complete. 3 out of 8 files\n",
      "50.0% complete. 4 out of 8 files\n",
      "62.5% complete. 5 out of 8 files\n",
      "75.0% complete. 6 out of 8 files\n",
      "87.5% complete. 7 out of 8 files\n",
      "100.0% complete. 8 out of 8 files\n",
      "100.0% complete. 1 out of 1 files\n",
      "100.0% complete. 1 out of 1 files\n",
      "Train, Validation, and Test arrays (with dicts) created in 16.14 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "# reduces number of classes\n",
    "labels = {k: v for k, v in sorted(label_counts.items(), key=lambda item: item[1], reverse=True) if v > 0}.keys()\n",
    "\n",
    "audioDataArrays(egmd_wavfiles[:n],auth_json,bucket_name,\n",
    "                representation='mel_stft',\n",
    "                labels=labels, \n",
    "                label_data=label_data, \n",
    "                label_dict=label_dict,\n",
    "                segment_length=5, sr=44100)\n",
    "\n",
    "\n",
    "# GCP output path formatted with number of egmd files\n",
    "#output_path = 'model_inputs/mdb_idmt_egmd{}'.format(n)\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "print('Train, Validation, and Test arrays (with dicts) created in', \n",
    "      round(time_end - time_start, 2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7rczncwsAX4"
   },
   "outputs": [],
   "source": [
    "# remove temporary local directory\n",
    "shutil.rmtree('model_inputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9aStwfplBn79"
   },
   "outputs": [],
   "source": [
    "# Unable to read file: raw-audio/eGMD/eGMD-wavfiles/drummer9_session1_26_rock_90_beat_4-4_26.midi.wav\n",
    "# Unable to read file: raw-audio/eGMD/eGMD-wavfiles/drummer9_session1_26_rock_90_beat_4-4_26.midi.wav\n",
    "# Unable to read file: raw-audio/eGMD/eGMD-wavfiles/drummer9_session1_29_rock_90_beat_4-4_23.midi.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'model_inputs/mdb_idmt_egmd_SFsr{}'.format(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil -m cp model_inputs/train_x.npy gs://{bucket_name}/{output_path}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "InputDataTransformation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
